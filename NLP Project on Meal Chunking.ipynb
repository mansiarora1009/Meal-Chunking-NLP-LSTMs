{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project on Meal chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing datasets\n",
    "meals = pd.read_csv(\"meals.tsv\", sep=\"\\t\")\n",
    "meals_by_row = pd.read_csv(\"meal_by_row.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use Glove vectors to convert the words into their vector representations. The following piece of code creates a dictionary of the words and their embeddings from the file downloaded from Stanford's Glove NLP page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.840B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            temp = line.split()\n",
    "            word = temp[0]\n",
    "            coefs = np.asarray(temp[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            pass\n",
    "#             print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts a sentence into its vector representation. I split the sentences on a space. Also, since commas can carry important information here, I included a space before every comma and then split the sentence. So the sentence \"Eggs, Sausage and coffee\" was converted to \"Eggs , Sausage and coffee\" which now consists of 5 different elements [\"Eggs\", \",\", \"Sausage\", \"and\", \"coffee\"], each of which are converted to their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()#.decode('utf-8')\n",
    "    words = re.sub(r'(?=[.,])(?=[^\\s^])', r' ', words)\n",
    "    words = words.split()\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            # if the word is not found, I just append a vector of 0's\n",
    "            M.append(np.zeros(300))\n",
    "    M = np.array(M)\n",
    "    return words, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating columns for words and their vectors\n",
    "meals[\"words\"] = [sent2vec(i)[0] for i in meals[\"text\"]]\n",
    "meals[\"vector\"] = [sent2vec(i)[1] for i in meals[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meals[\"vector\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting a sentence into a sequence of numbers\n",
    "# if a chunk is a single word, it is denoted with 4\n",
    "# if a chunk has more than one word, the start word is denoted with 2\n",
    "# and the end word is denoted with 3\n",
    "def sent2seq(ix):\n",
    "    sentence_part = meals[meals[\"idmealjsons\"] == ix][\"words\"]\n",
    "    items_part = meals_by_row[meals_by_row[\"mealjson_id\"] == ix][\"text\"].values\n",
    "    start_points = []\n",
    "    end_points = []\n",
    "    single_points = []\n",
    "    if len(meals[meals[\"idmealjsons\"] == i]) > 0:\n",
    "        for j in items_part:\n",
    "            words = str(j).lower()#.decode('utf-8')\n",
    "            words = re.sub(r'(?=[.,])(?=[^\\s^])', r' ', words)\n",
    "            words = words.split()\n",
    "            if len(words) > 1:\n",
    "                start_point = words[0]\n",
    "                end_point  = words[-1]\n",
    "                start_points.append(start_point)\n",
    "                end_points.append(end_point)\n",
    "            else:\n",
    "                single_points.append(words[0])\n",
    "        output = np.zeros(len(list(sentence_part)[0]))\n",
    "        # start points\n",
    "        boolean_start = [i in start_points for i in list(sentence_part)[0]]\n",
    "        output[boolean_start] = 2\n",
    "        boolean_end = [i in end_points for i in list(sentence_part)[0]]\n",
    "        output[boolean_end] = 3\n",
    "        boolean_single = [i in single_points for i in list(sentence_part)[0]]\n",
    "        output[boolean_single] = 4\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_output = []\n",
    "for i in range(1, meals[\"idmealjsons\"].max()+1):\n",
    "    meals_output.append((i,sent2seq(i)))\n",
    "output_df = pd.DataFrame(meals_output, columns = [\"id\", \"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the 2 datasets\n",
    "meals_combined = pd.merge(meals, output_df, left_on=\"idmealjsons\", right_on=\"id\", how=\"left\")\n",
    "vector_output = meals_combined[[\"idmealjsons\",\"vector\",\"output\"]]\n",
    "# checking if the length of the vector is the same as the output\n",
    "sum(sum([vector_output[\"vector\"].apply(len) != vector_output[\"output\"].apply(len)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# going to only consider vectors with length 12\n",
    "np.percentile(vector_output[\"vector\"].apply(len), q = 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5618"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_output[vector_output[\"vector\"].apply(len) < 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_output_12 = vector_output[vector_output[\"vector\"].apply(len) < 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idmealjsons</th>\n",
       "      <th>vector</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.41781, -0.035192, -0.12615, -0.21593, -0....</td>\n",
       "      <td>[4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.37412, -0.076264, 0.10926, 0.18662, 0.029...</td>\n",
       "      <td>[2.0, 3.0, 0.0, 4.0, 0.0, 4.0, 0.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[[-0.48838, 0.095686, 0.71593, -0.28175, 0.016...</td>\n",
       "      <td>[4.0, 0.0, 4.0, 0.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[-0.3449, -0.51021, 0.25005, -0.74105, 0.1379...</td>\n",
       "      <td>[2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[[-0.64583, -0.41343, 0.71445, 0.55511, 0.1908...</td>\n",
       "      <td>[4.0, 0.0, 4.0, 0.0, 4.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idmealjsons                                             vector  \\\n",
       "0            1  [[-0.41781, -0.035192, -0.12615, -0.21593, -0....   \n",
       "1            2  [[-0.37412, -0.076264, 0.10926, 0.18662, 0.029...   \n",
       "2            3  [[-0.48838, 0.095686, 0.71593, -0.28175, 0.016...   \n",
       "3            4  [[-0.3449, -0.51021, 0.25005, -0.74105, 0.1379...   \n",
       "4            5  [[-0.64583, -0.41343, 0.71445, 0.55511, 0.1908...   \n",
       "\n",
       "                                          output  \n",
       "0            [4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0]  \n",
       "1  [2.0, 3.0, 0.0, 4.0, 0.0, 4.0, 0.0, 2.0, 3.0]  \n",
       "2                      [4.0, 0.0, 4.0, 0.0, 4.0]  \n",
       "3                                     [2.0, 3.0]  \n",
       "4                      [4.0, 0.0, 4.0, 0.0, 4.0]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_output_12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mansiarora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\mansiarora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\mansiarora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\mansiarora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# padding with 0's on the right\n",
    "list_extras = 12 - vector_output_12[\"vector\"].apply(len)\n",
    "vector_output_12[\"pad_right\"] = [np.ones((i, 300)) for i in list_extras]\n",
    "vector_output_12[\"y_pad_right\"] = [np.ones(i) for i in list_extras]\n",
    "\n",
    "vector_output_12[\"padded_vector\"] = [np.concatenate(i) for i in np.hstack((vector_output_12[[\"vector\"]], vector_output_12[[\"pad_right\"]]))]\n",
    "vector_output_12[\"padded_output\"] = [np.concatenate(i) for i in np.hstack((vector_output_12[[\"output\"]], vector_output_12[[\"y_pad_right\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5618, 12, 300)\n",
      "(5618, 12)\n"
     ]
    }
   ],
   "source": [
    "# creating the final datasets with 12 timesteps, 300 features\n",
    "final = vector_output_12[[\"padded_vector\", \"padded_output\"]]\n",
    "X = np.array([np.concatenate(i) for i in final[\"padded_vector\"]]).reshape(len(final), 12, 300)\n",
    "y = np.array([i for i in final[\"padded_output\"]]).reshape(len(final), 12)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deciding length of validation set (has to be a multiple of 32 i.e. the batch size)\n",
    "int(len(X)*0.1/32)*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deciding the length of training set\n",
    "int(((len(X) - 1088)/32))*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mansiarora\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# converting y to categorical\n",
    "from keras.utils import to_categorical\n",
    "y = np.array([to_categorical(i, num_classes=5) for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5618, 12, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4512, 12, 300)\n",
      "(544, 12, 300)\n",
      "(544, 12, 300)\n",
      "(4512, 12, 5)\n",
      "(544, 12, 5)\n",
      "(544, 12, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[0:4512]\n",
    "X_val = X[4512:4512+544]\n",
    "X_test = X[4512+544:4512+544+544]\n",
    "y_train = y[0:4512]\n",
    "y_val = y[4512:4512+544]\n",
    "y_test = y[4512+544:4512+544+544]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose LSTMs because this is a sequence problem, where the history of the words matters to decide when to start and stop the subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (32, 12, 30)              39720     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (32, 12, 5)               155       \n",
      "=================================================================\n",
      "Total params: 39,875\n",
      "Trainable params: 39,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers.core import Dense\n",
    "from keras import optimizers\n",
    "model = Sequential()\n",
    "model.add(LSTM(30, batch_input_shape = (32, 12, 300), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(5, activation=\"softmax\"))) # softmax output layer\n",
    "adam = optimizers.Adam(0.0001)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_loss = []\n",
    "loss = []\n",
    "history = model.fit(X_train, y_train, shuffle=True,\n",
    "                    epochs = 50, batch_size=32, validation_data = (X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwl1EJIYU7iIiMaRwFxGJof8H8xgZnNyzdnoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9fa023d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], color=\"red\")\n",
    "plt.plot(history.history['val_loss'], color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 2, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[4, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4, 0, 3, 0, 4, 0, 4, 0, 4, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 0, 4, 0, 4, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[4, 0, 2, 3, 0, 4, 0, 4, 0, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 2, 3, 0, 2, 0, 3, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 0, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[2, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 2, 0, 3, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[2, 3, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[2, 3, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[4, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[2, 0, 3, 0, 0, 0, 3, 4, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 2, 3, 4, 2, 3, 4, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>[4, 0, 4, 3, 3, 0, 4, 0, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 2, 0, 3, 0, 4, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>[2, 3, 0, 0, 0, 3, 0, 2, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>[2, 0, 4, 0, 4, 0, 4, 0, 4, 3, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>[2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>[4, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>[4, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>[4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>[4, 0, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>[4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>[2, 3, 0, 2, 0, 3, 0, 2, 3, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 0, 2, 0, 3, 0, 2, 3, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>[2, 0, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>[2, 0, 3, 0, 4, 0, 4, 0, 4, 0, 4, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 0, 4, 0, 4, 0, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>[4, 0, 2, 3, 0, 4, 0, 4, 0, 2, 3, 1]</td>\n",
       "      <td>[4, 0, 2, 3, 0, 4, 0, 4, 0, 2, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>[2, 0, 4, 0, 4, 3, 0, 4, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 0, 4, 0, 2, 3, 0, 4, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>[2, 3, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>[2, 0, 0, 3, 3, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Predicted  \\\n",
       "0    [4, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1    [4, 0, 4, 0, 4, 0, 2, 1, 1, 1, 1, 1]   \n",
       "2    [4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3    [4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4    [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5    [4, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "6    [4, 0, 3, 0, 4, 0, 4, 0, 4, 1, 1, 1]   \n",
       "7    [4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "8    [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "9    [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "10   [4, 0, 2, 3, 0, 4, 0, 4, 0, 1, 1, 1]   \n",
       "11   [4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 0, 1]   \n",
       "12   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "13   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "14   [2, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "15   [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "16   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "17   [2, 3, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "18   [2, 3, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1]   \n",
       "19   [4, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "20   [4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "21   [2, 0, 3, 0, 0, 0, 3, 4, 1, 1, 1, 1]   \n",
       "22   [4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "23   [4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "24   [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "25   [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "26   [2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "27   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "28   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "29   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "..                                    ...   \n",
       "514  [4, 0, 4, 3, 3, 0, 4, 0, 1, 1, 1, 1]   \n",
       "515  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "516  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "517  [2, 3, 0, 0, 0, 3, 0, 2, 1, 1, 1, 1]   \n",
       "518  [2, 0, 4, 0, 4, 0, 4, 0, 4, 3, 1, 1]   \n",
       "519  [2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "520  [4, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "521  [4, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "522  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "523  [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "524  [4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "525  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "526  [4, 0, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "527  [4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "528  [2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "529  [2, 3, 0, 2, 0, 3, 0, 2, 3, 1, 1, 1]   \n",
       "530  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "531  [4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]   \n",
       "532  [2, 0, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1]   \n",
       "533  [2, 0, 3, 0, 4, 0, 4, 0, 4, 0, 4, 1]   \n",
       "534  [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "535  [4, 0, 2, 3, 0, 4, 0, 4, 0, 2, 3, 1]   \n",
       "536  [2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "537  [2, 0, 4, 0, 4, 3, 0, 4, 1, 1, 1, 1]   \n",
       "538  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "539  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "540  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "541  [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]   \n",
       "542  [2, 3, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1]   \n",
       "543  [2, 0, 0, 3, 3, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                   Actual  \n",
       "0    [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1    [4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]  \n",
       "2    [4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3    [2, 0, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4    [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "5    [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "6    [2, 0, 3, 0, 4, 0, 4, 0, 4, 1, 1, 1]  \n",
       "7    [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "8    [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "9    [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "10   [4, 0, 2, 3, 0, 2, 0, 3, 0, 1, 1, 1]  \n",
       "11   [4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 0, 1]  \n",
       "12   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "13   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "14   [4, 0, 2, 0, 3, 1, 1, 1, 1, 1, 1, 1]  \n",
       "15   [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "16   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "17   [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "18   [2, 3, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1]  \n",
       "19   [4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "20   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "21   [2, 3, 2, 3, 4, 2, 3, 4, 1, 1, 1, 1]  \n",
       "22   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "23   [2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "24   [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "25   [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "26   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "27   [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "28   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "29   [2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "..                                    ...  \n",
       "514  [4, 0, 2, 0, 3, 0, 4, 0, 1, 1, 1, 1]  \n",
       "515  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "516  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "517  [2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1]  \n",
       "518  [4, 0, 4, 0, 4, 0, 4, 0, 2, 3, 1, 1]  \n",
       "519  [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "520  [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "521  [4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "522  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "523  [2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "524  [4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "525  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "526  [4, 0, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "527  [4, 0, 4, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "528  [2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "529  [2, 3, 0, 2, 0, 3, 0, 2, 3, 1, 1, 1]  \n",
       "530  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "531  [4, 0, 4, 0, 4, 0, 4, 1, 1, 1, 1, 1]  \n",
       "532  [2, 0, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1]  \n",
       "533  [2, 0, 3, 0, 4, 0, 4, 0, 4, 0, 4, 1]  \n",
       "534  [4, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "535  [4, 0, 2, 3, 0, 4, 0, 4, 0, 2, 3, 1]  \n",
       "536  [2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "537  [4, 0, 4, 0, 2, 3, 0, 4, 1, 1, 1, 1]  \n",
       "538  [2, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "539  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "540  [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "541  [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "542  [2, 0, 3, 0, 4, 1, 1, 1, 1, 1, 1, 1]  \n",
       "543  [2, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[544 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results \n",
    "# 2 means start the chunk, 3 means end the chunk, 4 means it's a single word\n",
    "meal_ids = vector_output_12[4512+544:4512+544+544][\"idmealjsons\"]\n",
    "meals_test = pd.DataFrame(meals[meals[\"idmealjsons\"].isin(meal_ids)][\"words\"]).reset_index(drop=True)\n",
    "actual_pred = pd.DataFrame([(np.argmax(y_pred[i], axis=1),np.argmax(y_test[i], axis=1)) for i in range(len(y_pred))], columns = [\"Predicted\", \"Actual\"])\n",
    "pd.concat((actual_pred, meals_test), axis=1)[[\"Predicted\", \"Actual\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by scrolling of the examples, the model is able to accurately detect when to start and stop the chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
